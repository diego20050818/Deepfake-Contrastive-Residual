# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader
# from torch.utils.tensorboard.writer import SummaryWriter
# import torchvision.transforms as transforms
# import torchvision.datasets as datasets
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
# import numpy as np
# from loguru import logger
# import matplotlib.pyplot as plt
# import seaborn as sns
# from PIL import Image
# import os
# from typing import Dict, List, Tuple, Optional
# import argparse
# from tqdm import tqdm
# import warnings
# import matplotlib

# warnings.filterwarnings("ignore")

# class ModelValidator:
#     def __init__(self, dataloader: DataLoader, 
#                  class_names: List[str], 
#                  chechpoint_info:dict,
#                  model=None, 
#                  model_path: Optional[str] = None, 
#                  log_dir: str = 'runs/validation',
#                  ):
#         """
#         åˆå§‹åŒ–æ¨¡å‹éªŒè¯å™¨
        
#         Args:
#             dataloader: éªŒè¯æ•°æ®çš„ DataLoader
#             class_names: ç±»åˆ«åç§°åˆ—è¡¨
#             model: å·²ç»åˆå§‹åŒ–çš„æ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
#             model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›äº†modelåˆ™ä¸éœ€è¦ï¼‰
#             log_dir: tensorboardæ—¥å¿—ä¿å­˜è·¯å¾„
#         """
#         self.checkpoint_info = chechpoint_info
#         self.name = self.checkpoint_info.get('name')
#         data_name_value = self.checkpoint_info.get('dataset')
#         if isinstance(data_name_value, list):
#             self.data_name = " | ".join(str(item) for item in data_name_value)
#         elif data_name_value is None:
#             self.data_name = "Unknown Dataset"
#         else:
#             self.data_name = str(data_name_value)

#         self.dataloader = dataloader    
#         self.class_names = class_names
#         self.model_path = model_path
#         self.log_dir = log_dir
#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#         self.writer = SummaryWriter(log_dir=log_dir)
        
#         # è®¾ç½®æ¨¡å‹
#         if model is not None:
#             self.model = model
#             self.model.to(self.device)
#             self.model.eval()
#         elif model_path is not None:
#             self.load_model()
#         else:
#             raise ValueError("å¿…é¡»æä¾›modelå®ä¾‹æˆ–model_path")
        
#     def load_model(self):
#         """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
#         checkpoint = torch.load(self.model_path, map_location=self.device) # type:ignore
        
#         # å¤„ç†ä¸åŒç±»å‹çš„checkpoint
#         if isinstance(checkpoint, dict):
#             # å°è¯•å¤šç§å¯èƒ½çš„é”®åæ¥è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
#             state_dict = None
#             model_keys = ['model_state_dict', 
#                         #   'state_dict', 
#                         #   'model', 
#                         #   'net'
#                           ]
            
#             for key in model_keys:
#                 if key in checkpoint:
#                     state_dict = checkpoint[key]
#                     break
            
#             if state_dict is not None:
#                 # å¦‚æœå·²ç»æœ‰æ¨¡å‹å®ä¾‹ï¼Œç›´æ¥åŠ è½½æƒé‡
#                 if hasattr(self, 'model') and self.model is not None:
#                     self.model.load_state_dict(state_dict)
#                 else:
#                     raise ValueError("æœªæä¾›æ¨¡å‹å®ä¾‹ï¼Œè¯·åœ¨åˆå§‹åŒ–ModelValidatoræ—¶æä¾›modelå‚æ•°")
#             else:
#                 # å¦‚æœcheckpointä¸­æ²¡æœ‰æ˜æ˜¾çš„çŠ¶æ€å­—å…¸ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨æ¨¡å‹
#                 logger.warning('checkpointä¸­æ²¡æœ‰æ˜æ˜¾çš„çŠ¶æ€å­—å…¸ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨æ¨¡å‹')
#                 self.model = checkpoint.get('model', checkpoint)
#         else:
#             # ç›´æ¥åŠ è½½æ¨¡å‹å¯¹è±¡
#             self.model = checkpoint
        
#         # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶å¤„äºè¯„ä¼°æ¨¡å¼
#         if hasattr(self.model, 'to'):
#             self.model = self.model.to(self.device)
#         if hasattr(self.model, 'eval'):
#             self.model.eval()
            
#     def validate(self) -> Dict[str, float]:
#         """
#         æ‰§è¡Œæ¨¡å‹éªŒè¯å¹¶è®¡ç®—å„é¡¹æŒ‡æ ‡
#         ä¿®å¤äº† batch ä¸æ•´é™¤å¯¼è‡´çš„ numpy æŠ¥é”™ï¼Œå¹¶å¢åŠ äº†è¯¦ç»†çš„æ¦‚ç‡å¯¹æ¯”æ—¥å¿—
#         """
#         # ä½¿ç”¨åˆ—è¡¨æ”¶é›†æ¯ä¸ª batch çš„ç»“æœï¼Œæœ€åå†æ‹¼æ¥
#         # é¿å…ç›´æ¥ append åˆ°ä¸€ä¸ªå¤§ list ç„¶åè½¬ numpy å¯¼è‡´çš„ç»´åº¦é”™è¯¯
#         batch_preds_list = []
#         batch_labels_list = []
#         batch_probs_list = []
        
#         # æ·»åŠ è¿›åº¦æ¡
#         progress_bar = tqdm(self.dataloader, desc="Validating", leave=False)
        
#         self.model.eval() # ç¡®ä¿æ˜¯ eval æ¨¡å¼
#         with torch.no_grad():
#             for inputs, labels in progress_bar:
#                 inputs, labels = inputs.to(self.device), labels.to(self.device)
                
#                 # --- 1. è·å–æ¨¡å‹è¾“å‡º ---
#                 try:
#                     # ä½ çš„æ¨¡å‹è¿”å› (logits, repr)
#                     outputs_tuple = self.model(inputs)
#                     if isinstance(outputs_tuple, tuple):
#                         logits = outputs_tuple[0] # å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼šlogits
#                     else:
#                         logits = outputs_tuple
#                 except Exception as e:
#                     print(f"Model output error: {e}")
#                     raise e
                
#                 # --- 2. è½¬æ¢ä¸ºæ­£æ ·æœ¬æ¦‚ç‡ ---
#                 # Logits -> Sigmoid -> Probability (0.0 ~ 1.0)
              
#                 probs = torch.sigmoid(logits) 
                
#                 # --- 3. ç”Ÿæˆç¡¬é¢„æµ‹ (0 æˆ– 1) ---
#                 preds = (probs > 0.5).float()

#                 # --- 4. æ”¶é›†æ•°æ® (ä¿æŒåœ¨ CPU ä¸Š) ---
#                 # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥å­˜ numpy æ•°ç»„ï¼Œè€Œä¸æ˜¯ extend åˆ—è¡¨
#                 batch_probs_list.append(probs.cpu().numpy())
#                 batch_labels_list.append(labels.cpu().numpy())
#                 batch_preds_list.append(preds.cpu().numpy())
                
#                 # æ›´æ–°è¿›åº¦æ¡
#                 progress_bar.set_postfix({'Batch': inputs.size(0)})
        
#         # --- 5. å®‰å…¨æ‹¼æ¥ (Fix: è§£å†³ inhomogeneity æŠ¥é”™) ---
#         # ä½¿ç”¨ concatenate å¤„ç†æœ€åä¸€ä¸ª batch å¤§å°ä¸ä¸€è‡´çš„é—®é¢˜
#         all_probs = np.concatenate(batch_probs_list, axis=0)
#         all_labels = np.concatenate(batch_labels_list, axis=0)
#         all_preds = np.concatenate(batch_preds_list, axis=0)

#         # all_probs = all_preds.squeeze(1) 
        
#         # --- 6. æ‰“å°ç›´è§‚å¯¹æ¯” ---
#         print("\n" + "="*40)
#         print("ğŸ” Probability vs Label Check (Top 10 samples)")
#         print(f"{'Probability (Positive)':<25} | {'Label':<10} | {'Correct?'}")
#         print("-" * 50)
#         for i in range(min(10, len(all_labels))):
#             p = all_probs[i]
#             l = all_labels[i]
#             # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
#             is_correct = "âœ…" if (p > 0.5) == (l == 1) else "âŒ"
#             print(f"{p:.4f} ({(p*100):.1f}%) {'':<12} | {int(l):<10} | {is_correct}")
#         print("="*40 + "\n")

#         # --- 7. è®¡ç®—æŒ‡æ ‡ (ä¿æŒåŸæœ‰é€»è¾‘) ---
#         metrics = {}
#         metrics['accuracy'] = accuracy_score(all_labels, all_preds)
#         metrics['precision'] = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
#         metrics['recall'] = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
#         metrics['f1_score'] = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
        
#         # äºŒåˆ†ç±»ç‰¹æœ‰æŒ‡æ ‡
#         if len(self.class_names) == 2:
#             try:
#                 metrics['auc'] = roc_auc_score(all_labels, all_probs)
                
#                 # è®¡ç®—æœ€ä½³é˜ˆå€¼
#                 fpr, tpr, thresholds = roc_curve(all_labels, all_probs)
#                 optimal_idx = np.argmax(tpr - fpr)
#                 optimal_threshold = thresholds[optimal_idx]
#                 metrics['optimal_threshold'] = optimal_threshold
                
#                 optimal_preds = (all_probs >= optimal_threshold).astype(int)
#                 metrics['optimal_accuracy'] = accuracy_score(all_labels, optimal_preds)
#             except Exception as e:
#                 print(f"Warning: Could not calculate ROC/AUC: {e}")
#                 metrics['auc'] = 0.0
        
#         metrics['error_rate'] = 1 - metrics['accuracy']
        
#         return metrics, all_labels, all_preds, all_probs # type:ignore
    
#     def plot_confusion_matrix(self, labels: np.ndarray, preds: np.ndarray) -> plt.Figure: # type:ignore
#         """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
#         cm = confusion_matrix(labels, preds)
#         fig, ax = plt.subplots(figsize=(8, 6))
#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
#                    xticklabels=self.class_names, 
#                    yticklabels=self.class_names,
#                    ax=ax)
#         ax.set_xlabel('Predicted Labels')
#         ax.set_ylabel('True Labels')
#         ax.set_title(f'Confusion Matrix \nmodel:{self.name}\ndataset:{self.data_name}',
#                      pad=15)   
#         plt.tight_layout()
#         return fig
    
#     def plot_roc_curve(self, labels: np.ndarray, probs: np.ndarray) -> plt.Figure: # type:ignore
#         """ç»˜åˆ¶ROCæ›²çº¿"""
#         fig, ax = plt.subplots(figsize=(8, 6))
        

#         if probs.ndim == 1:
#             # å¦‚æœprobsæ˜¯ä¸€ç»´çš„ï¼Œç›´æ¥ä½¿ç”¨
#             fpr, tpr, _ = roc_curve(labels, probs)
#             auc_score = roc_auc_score(labels, probs)
#         else:
#             # å¦‚æœprobsæ˜¯äºŒç»´çš„ï¼Œä½¿ç”¨ç¬¬äºŒåˆ—ï¼ˆæ­£ç±»ï¼‰
#             fpr, tpr, _ = roc_curve(labels, probs[:, 1])# BUG
#             auc_score = roc_auc_score(labels, probs[:, 1])
        
#         ax.plot(fpr, tpr, color='darkorange', lw=2, 
#                 label=f'ROC curve (AUC = {auc_score:.2f})')
#         ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
#                 label='Random classifier')
#         ax.set_xlim([0.0, 1.0])# type:ignore
#         ax.set_ylim([0.0, 1.05])# type:ignore
#         ax.set_xlabel('False Positive Rate')
#         ax.set_ylabel('True Positive Rate')
#         ax.set_title(f'Receiver Operating Characteristic (ROC) Curve\nmodel:{self.name}\ndataset:{self.data_name}',
#                         pad=15,
#                         )  
#         ax.legend(loc="lower right")
#         ax.grid(True)

        
#         plt.tight_layout()
#         return fig
    
#     def visualize_samples(self, num_samples: int = 16):
#         """å¯è§†åŒ–ç¤ºä¾‹å›¾ç‰‡"""
#         # è·å–ä¸€æ‰¹æ•°æ®ç”¨äºå¯è§†åŒ–
#         data_iter = iter(self.dataloader)
#         images, labels = next(data_iter)
        
#         # å°†æ ‡å‡†åŒ–çš„å›¾åƒè¿˜åŸï¼ˆè¿™é‡Œå‡è®¾ä½¿ç”¨äº†æ ‡å‡†çš„ImageNetå½’ä¸€åŒ–ï¼‰
#         mean = np.array([0.485, 0.456, 0.406])
#         std = np.array([0.229, 0.224, 0.225])
        
#         fig, axes = plt.subplots(4, 4, figsize=(12, 12))
#         axes = axes.ravel()
        
#         for i in range(min(num_samples, len(images))):
#             img = images[i].cpu().numpy().transpose(1, 2, 0)
#             img = np.clip(std * img + mean, 0, 1)  # åæ ‡å‡†åŒ–
            
#             axes[i].imshow(img)
#             axes[i].set_title(f'True: {self.class_names[labels[i]]}')
#             axes[i].axis('off')
            
#         plt.tight_layout()
#         fig.suptitle(f"Sample images\n{self.name}\n{self.data_name}", fontsize=16, y=0.98)
#         plt.subplots_adjust(top=0.85)
#         return fig
    
#     def log_to_tensorboard(self, metrics: Dict[str, float], 
#                           labels: np.ndarray, preds: np.ndarray, 
#                           probs: np.ndarray):
#         """å°†ç»“æœè®°å½•åˆ°tensorboard"""
#         # è®°å½•æ ‡é‡æŒ‡æ ‡
#         for metric_name, value in metrics.items():
#             self.writer.add_scalar(f'Validation/{metric_name}', value, 0)
        
#         # è®°å½•æ··æ·†çŸ©é˜µ
#         cm_fig = self.plot_confusion_matrix(labels, preds)
#         self.writer.add_figure('Validation/Confusion_Matrix', cm_fig, 0)
        
#         # è®°å½•ROCæ›²çº¿
#         roc_fig = self.plot_roc_curve(labels, probs)
#         self.writer.add_figure('Validation/ROC_Curve', roc_fig, 0)
        
#         # è®°å½•ç¤ºä¾‹å›¾ç‰‡
#         sample_fig = self.visualize_samples()
#         self.writer.add_figure('Validation/Sample_Images', sample_fig, 0)
        
#         # åˆ›å»ºæŒ‡æ ‡è¡¨æ ¼
#         metric_table = f"#### model:{self.name}\n#### dataset:{self.data_name}\n"
#         metric_table += "| Metric | Value |\n|--------|-------|\n"       
#         for name, value in metrics.items():
#             metric_table += f"| {name} | {value:.4f} |\n"
        
#         # è®°å½•æ–‡æœ¬æ ¼å¼çš„æŒ‡æ ‡
#         metric_text = "Metrics logged to TensorBoard:\n"
#         for name, value in metrics.items():
#             metric_text += f"{name}: {value:.4f}\n"
#             print(f"{name}: {value:.4f}")
        
#         self.writer.add_text('Validation/Metrics_Table', metric_table, 0)
#         self.writer.add_text('Validation/Metrics_Text', metric_text, 0)
    
#     # @logger.catch()
#     def run_validation(self):
#         """è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹"""
#         logger.info("Running validation...")
#         metrics, labels, preds, probs = self.validate()
        
#         logger.info("Logging to TensorBoard...")
#         self.log_to_tensorboard(metrics, labels, preds, probs) # type:ignore
        
#         logger.info(f"Validation completed. Results saved to {self.log_dir}")
#         self.writer.close()

#         logger.success("validation success")